[Record]
read_weight_path: 
write_weight_folder: ../../train/0524_graphAE_capsules/weight_30/
write_tmp_folder: ../../train/0524_graphAE_capsules/tmp_30/
logdir: ../../train/0524_graphAE_capsules/log_30/



[Params] 
lr: 0.001

batch: 16

w_pose: 0.8
w_laplace: 0.2

augment_data: 0

weight_decay: 0.00000
lr_decay: 0.99
lr_decay_epoch_step: 1


start_epoch: 0
epoch: 601
evaluate_epoch: 2

perpoint_bias: 0


template_ply_fn: ../../data/CAPSULES/template.ply


point_num: 2562

pcs_train: ../../data/CAPSULES/train.npy

pcs_evaluate: ../../data/CAPSULES/eval.npy

pcs_test: ../../data/CAPSULES/test.npy

connection_folder:  ../../train/0524_graphAE_capsules/ConnectionMatrices/

initial_connection_fn:../../train/0524_graphAE_capsules/ConnectionMatrices/_pool0.npy

connection_layer_lst: ["pool0", "pool1", "pool2","pool3", "pool4", "pool5", "pool6", "pool7", "unpool7","unpool6", "unpool5","unpool4","unpool3","unpool2","unpool1", "unpool0"]

## pool and unpool layer's output channel number should be the same as the previous layer
## we can do more fine tuning
channel_lst:          [ 32, 32, 64, 64, 128, 128, 9, 1, 9, 128, 128,64, 64,32, 32,3] 

weight_num_lst:      [ 6,0,    6,0,    6,0,      6,0,   0,6, 0,6,0,6,0,6]

## weight_num_lst:       [9,0, 9,0, 9,0, 9,0,   0,9, 0,9,0,9,0,9]

## 0 for conv only, 1 for (un)pool and residual layer, 0.X for residual block: (1-0.X)*conv+0.X*res 
residual_rate_lst:    [0,1,  0,1,  0,1,  0,1,   1,0,  1,0,  1,0,  1,0]






